\chapter{Methods}
I want the framework and methodology we use to be easy to use and understand, so that it can easily be discussed among clinicians and other people who don’t have a statistics or mathematics background. On the same token, I want the methods and theory to be sound from a statistical point of view. For the three parts that we are combining, there are a lot of differing theories and paradigms. I aim to pick the ones that optimize ease of understanding and power of results. Along the way, we will also develop new methods and validation tools, as well as apply the existing theory to situations that it has not previously been applied to.
\section{Multiple Imputation}
\label{sec:MI}
Our first decision comes as to what paradigm we should impute under. It should be noted that as long as we can produce valid imputations, the choice of method does not matter. However, since the base of our analysis starts with imputation, we need to make sure that we pick a good method. Everything that follows in the analysis is dependent on our imputed data, so it is necessarily the case that bad imputation will lead to poor results (be it bias, high variability, loss in statistical power, etc.). The methods we will discuss here are geared towards and motivated by cancer research, but can be easily adapted to other areas.

There are two main divisions in modern multiple imputation, and they are joint modelling and full conditional specification. Both have their own flaws and advantages. I will describe both, and then explain why full conditional specification is better suited for cancer research.

Before we get in to the imputation models, we need to have a firm understanding of missing data concepts. They take up quite a bit of space to explain, but they are fundamental concepts. If you are unfamiliar with them, please read appendix \ref{app:apdx}  before reading further.

In joint modelling (JM), we assume that the missing data mechanism is ignorable and that the data can be described by a multivariate distribution on the rows of the data (specified by the user). We then draw imputations from the joint distribution of the unknowns for the rows, given what we do know and their associated unknown parameter of the imputation model. !!!!Example here!! Since we don’t know the true model parameters, we need to estimate them. This is often done by a data augmentation algorithm \cite{VanBuuren2012}. There has been extensive research on using the normal model for this, and research shows that it even performs well under data that has strong non-normality. An obvious issue arises when we have discrete or categorical data. There has been much debate in the literature about what to do with it. Some authors argue that you should just impute under a continuous distribution and round, and others suggest using distributions that are more suited for categorical data \cite{VanBuuren2012}. There are a few R packages for joint modelling imputation include Amelia \cite{Honaker2015}, norm \cite{norm2015} and cat \cite{cat2015}.
On the other hand, there is fully conditional specification (FCS). In this paradigm, missing data is imputed on a variable by variable case (on the columns), based off of a specification of the imputation model for each imputed variable. These full conditionals should factor to specify the joint distribution. In the JM setting, we must give a k dimensional model, however in the FCS setting, we must give k one dimensional models. We are trying to sample from
$$P(Y,X,R|\theta)$$
By sampling from the full conditionals
$$P(Y_j|X,Y_{-j},R,\phi_j)$$
In this notation, $Y_-j$ means all of the columns with missing data except for j, and X is the fully observed columns (which could possibly be empty). 
One of the major flaws of this method is that in order for there to be a guarantee that we are sampling from the correct distribution, we need to ensure that our full conditionals are compatible, i.e that they factor into the proper joint. This is very hard to check in practice, but studies have shown that even when the models are highly incompatible, FCS methods are very robust \cite{VanBuuren2006}. But despite this, FCS allows us much more flexibility than JM does.
This is the framework for FCS, and there are many different implementations of it. The three most common ones are the additive linear regression approach implemented in the Harrell package, and package mi
We are going to have to specify something, there is no escaping that, but I think that it is easier for the average person (especially a clinician) to be able to define a single distribution and model rather than to guess at a multivariate. In addition, in the survival analysis setting, we will naturally have time variables be only positive, and some binary indicators, whereas others can take any value. Trying to fit a parametric distribution with these stipulations will be very hard if not impossible, so we will be relegated to using a general distribution (like the normal), which will certainly elicit a poor fit. So, the fully conditional specification will be our choice.  In an ideal world, we would have complete data, and would not need to resort to imputation. But since we don’t have complete data, we must choose one method and accept its strengths and weaknesses.
Now that we have chosen the paradigm, we need to select an implementation of it. Many exist (such as MICE \cite{VanBuuren2011}, mi \cite{Su2011}, etc.). I wanted to select the implementation that combined ease of use, understanding, and programming. What I decided upon was a method called MICE- Multiple Imputation by chained equations \cite{VanBuuren2011} MICE is an FCS MCMC method that under compatibility, is a Gibbs sampler, where we obtain samples from the joint by sampling from the full conditionals. The user defines the full conditionals, so it is possible that the joint may only exist implicitly, and not actually have a functional form. 
In order to use mice, we must have that the missingness in our data to be MCAR or MAR. It can work with MNAR data, but it requires some extra modelling assumptions. This is a seldom observed case in practice, so the interested reader may check \cite{VanBuuren2011}  section 6.2 for a detailed look at this. 
The mice algorithm in pseudocode here!! Figure out how to do this!!
Once we have the correct assumptions, we need to set up our full conditionals imputation models. This may take a while for large datasets, but the extra time spent will ensure a better model. We choose what predictors will go into imputation, and what method to use (regression, predictive mean matching, logistic regression, etc.).  We should choose predictor variables that are somewhat correlated with the missing data, as well as include the covariates that we are doing inference on, as to avoid bias. For variables that are derived from others, we impute the others and then compute that variable, in a process known as passive imputation.  Since mice is an iterative process, we must choose how many iterations we will do until convergence. The older literature suggests only 5 is enough (source), but with modern computation, we can easily exceed this, even with large data. As well, we need to decide how many datasets to impute. The early literature argued that 5 will due, but more is better, since it will cut down on simulation error (find where I wrote up the reasons why). Modern literature suggests X.
?? rhat test??We need to verify that our imputations are valid once we complete them. The overarching idea that we need to pay attention to is “does the data look like it could have been real data”. We can assess this in many ways, including density plots, box and whisker plots, etc. There is not much in the imputation literature about statistical tests to check for convergence, but !!!!!work on this!!

Once we have m imputed datasets, we may run any valid analysis (regression, computing any statistic) on each imputed dataset INDIVIDUALLY, treating each of the m datasets as if it was complete. We may then use Rubin’s rules \cite{Rubin1987} to pool our estimates. This will give us a point estimate, as well as the proper variance for the quantity we have in mind.  Rubin’s rules are essential for using multiply imputed datasets, so we need to investigate them thoroughly.
Rubin’s rules are a set of rules that guide us in making inference from multiply imputed data. It involves three parts. The first is getting an estimate of the population estimand Q, we do so by taking the average of the MI sample estimands ($\hat{Q}_i$) to get the MI estimate $\bar{Q}$.
$$\bar{Q}=\frac{1}{m}\sum_{i=1}^{m}\hat{Q}_i$$,
Where $\hat{Q}$ is the estimand evaluated from the data in the $i^{th}$ dataset.
The estimates are not set, and there is variance associated with them. The first form of variance is the “within” variance, or the variance or each estimate. We can get an MI estimate of this quantity by doing
$$\bar{U}=\frac{1}{m}\sum_{i=1}^{m}\bar{U}_i$$
Where $\bar{U}_i$ is the $i^{th}$ datasets variance
The other form of the variance is the “between datasets” variance. This is the variance associated with the fact that we have missing data. It is given by
$$B=\frac{1}{m-1}\sum_{i=1}^{m}(\hat{Q}_i-\bar{Q})$$
The total variance for our estimand is given by 

$$T=\bar{U}+B +\frac{B}{M}$$
The last term is our simulation variance, and its existence is proven by Rubin in \cite{Rubin1987}
 The theory is rooted in the assumption that whatever we are trying to pool is asymptotically normally distributed with mean Q and variance U. We don’t have these population values, so we must use what we have from the sample, namely $\bar{Q}$ and $\bar{U}$. With this assumption, we know that
$$\frac{Q-\bar{Q}}{\sqrt{T}}\sim t_{\nu}$$ 
And the degrees of freedom is proven to be equal to !! !! Put source when I get the source for it from anderson
$$\nu=\frac{\nu_{old}\nu_{obs}}{\nu_{old}+\nu_{obs}}$$
Where $\nu_{obs}=\frac{\nu_{com}+1}{\nu_{com}+3}\nu_{com}(1-\frac{B + B/m}{T})$
And $\nu_{old}=\frac{m-1}{(\frac{B + B/m}{T})^2}$
Rubin’s rules assume normality, so if our statistic in mind is not asymptotically normal, we need to transform it towards normality before we pool. It should also be noted that we have discussed the univariate case, but this easily extends to the multivariate case. We now have a powerful framework to get valid inference from multiply imputed data.
%start here

\section{Survival analysis}
\label{sec:Surv}
Now that we have the multiple imputation datasets created, we may run our analysis. As a general rule of thumb, we should run our desired analyses on the available data first, to get an idea of what to expect. Because we are working with cancer data, we are interested in some basic survival quantities (Kaplan-Meier survival estimates for survival function, Cox regression to determine hazard ratio), as well as some more advanced ones (cumulative incidence for survival in the competing risks setting). Following Rubin’s rules, we run the individual analyses on each of the $m$ datasets, and then pool our results. 

Analysis for the Kaplan-Meier estimate is easy individually. All we need to do is clearly define what groups are, and what constitutes an event of interest. As well, we should verify that we are not in a competing risks situation. This setting will be discussed later. Once we have checked all of these, we can run the Kaplan-Meier curve on each of the $m$ datasets. Now, we could pool these estimates , but that would be ill advised, because the Kaplan-Meier curve is not normally distributed. To get around this, it has been proposed by Marshall et. al to take the complimentary log log transformation of the survival estimates before pooling \cite{Marshall2009}. We can make this transformation, and then pool our results. An interesting situation may arise where some of the survival curves may end before others. This is the result of a person with a long survival time being put in different groups through the imputation. We can deal with this by either extending the last observed Kaplan-Meier estimate out until the last time, or truncating all of the imputed curves at the minimum time of last event. *** figure out which one, although I think the latter**. Once we have our pooled estimates, we can back transform them using inverse of the complimentary log log transformation.


Now, we will have a pooled estimate of the true survival curve. In the typical setting, we might want to look to see if these curves are similar to each other. We would do this with a log-rank test. However, we should not be deceived. We have an averaged survival curve. It is not constructed in the same way that a regular Kaplan-Meier curve is, so we cannot get the quantities that we would need to compute the log rank test. However, we can still get the pooled log rank test. To do so, we can do one of two things. The first is to run the logrank test on each of the datasets and then pool via Rubin’s rules. This is the logical way to do it, but the logrank statistic is not normally distributed, and no obvious transformation comes to mind. Another option is to run a cox regression on just the group in question. Cox regression approximates the log rank test in this situation, and the cox regression coefficients are in fact normally distributed.
%start here
This works fine for the Cox model, as the regression coefficients are asymptotically normally distributed (is this source or is it known?). In many of the MI packages in R, there are functions to model and pool with Cox regression. Care needs to be taken to ensure that our analysis is valid (i.e. that we do in fact have proportional hazards). We can go about this in two ways. The first is to check the assumptions on each individual model fit to each dataset. This may prove to be an arduous task, but with graphical tools such as shiny, this isn’t too bad. We may also “stack” our multiple imputations on top of each other, and run one huge model (thus our MI data are acting as replicates). This will produce unbiased estimates, but the standard errors will be too low. So, this tool may be used as a graphical check, but the results from the inference of this cannot be trusted(source for all of the above). Once we have verified that the model follows the assumptions, we may trust its results, and perform inference on the parameters using the obtained total variance.
We now would like to calculate the Kaplan-Meier curve for a subset of the population.  While this is a pretty simple task in the complete case setting, it is a little more difficult in the multiple imputation setting. One of the main tasks that clinicians are interested in is the median survival time (the smallest time that the survival function is less than .5), specifically, the variance at the median. The median is a much better estimate of survival than the mean is, because in survival analysis, the time to event is typically right skewed, so the mean survival time is almost certainly not relevant to the typical patient/user of the km plot. We could get an unbiased estimate of the median via stacking the imputations and then running a km model, but this will give us false confidence, because our sample size is greatly inflated.  The correct way to go about this is to run a Kaplan Meier curve on each of the imputed datasets, and then pool the estimates. We will run into two problems here though. The first is that the Kaplan Meier curve is not normally distributed, so we will need to pretransform the data before pooling towards normality. [source] claims the complimentary log log transformation helps with this, and this transformation was implemented in [prostate] paper  successfully.  The second issue is that the variance at a point is usually computed under greenwoods formula, which depends on the order of the data. We can stack, but this will lead to too small of an estimate. As well, we could pool on greenwoods formula, but this would lead to too big of changes between the data sets (by construction of the greenwood estimator). The solution for this issue is to derive the variance of the median by the “reflection method”, is look at all of our Kaplan Meier curves, and take the median to be the average median, and take the confidence interval to be the time the first and last curve cross the 50 line * *work on this*. 
The last thing that we might be interested in is the cause specific hazard, and the cumulative incidence function. Work on this a lot.

\section{Propensity Score Analysis}
\label{sec:Propscore}
Now that we have laid down the theory for analyzing the survival section for clinical relevance, we can move on to the causal analysis part. While there is a lot of preparatory work that goes into the theory of it, the results that can be obtained using causal analysis and propensity scores is much stronger than conventional analysis.  As well, with causal analysis, we get a cause and effect result, which is in tune with what the general population believes that results should be. Propensity score methods are an easy to understand yet powerful tool. Our overall goal is to estimate the average treatment effect in a setting where the initial study was not a completely randomized experiment.
We will need to make a few decisions along the way. Our very first decision comes when deciding how to use the propensity score. We can either choose to match or stratify on the propensity score. The use of doing either is justified in Rubin’s paper !!Rubin 1983!!. Both will help us, but matching is easier for the layperson to understand, and easier to implement, so we shall use that.
Our next decision comes as to how to use matching in the multiple imputation setting. The stacking method described before would obviously be inappropriate, as complete records would always be matched to themselves, and imputed values would often be matched with their selves from another imputation. As well, we would have a falsely inflated data, giving us confidence in our matches where we should not have any.
Two methods are described in Robin and Mitra !!sauce!! about how to do this. In the first method, propensity score matching is done within each MI dataset (known as within matching). This, we will get m estimates of the treatment effect to which we will average. The other method, known as the across method takes the average propensity score for each individual and estimates the treatment effect in that manner. Both methods have their pros and cons, but Mitra and Reiter show that the across method limits bias more than the within one does. I need to determine what to use, because I might want to use inverse PS weighting in the cox model.
