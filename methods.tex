\chapter{Methods}
I want the framework and method we use to be easy to use and understand, so that it can easily be discussed among clinicians and other people who don’t have a statistics or mathematics background. On the same token, I want the methods and theory to be sound from a statistical point of view. For the three parts that we are combining, there are a lot of differing theories and paradigms. I aim to pick the ones that optimize ease of understanding and power of results, with the motivating example being cancer survival data. Throughout this section, we will need to make decisions as to what methods and analyses we will use. Whenever a decision is made, I explain why it was chosen, and alternative methods that could also be used.
\section{Multiple Imputation}
\label{sec:MI}
It should be clear that multiple imputation is the preferred method do deal with missing data, so our first decision comes as to what paradigm we should impute under. It should be noted that as long as we can produce valid imputations, the choice of method does not matter. However, since the base of our analysis starts with imputation, we need to make sure that we pick a good method. Everything that follows in the analysis is dependent on our imputed data, so it is necessarily the case that bad imputation will lead to poor results (be it bias, high variability, loss in statistical power, etc.).

There are two main divisions in modern multiple imputation, and they are joint modelling and full conditional specification. Both have their own flaws and advantages. I will describe both, and then explain why full conditional specification is better suited for cancer research.

Before we get in to the imputation models, we need to have a firm understanding of missing data concepts. They take up quite a bit of space to explain, but they are fundamental concepts. If you are unfamiliar with them, please read appendix \ref{app:apdx} before reading further.

In joint modelling (JM), we assume that the missing data mechanism is ignorable and that the data can be described by a multivariate distribution (specified by the user) on the rows (missing data pattern) of the data. Then, we run a sampler that draws imputations from the specified model, and updates model parameters. Since we don’t know the true model parameters, we need to estimate them. This is often done by a data augmentation algorithm \cite{VanBuuren2012}.  A pseudocode example will help better clarify the steps.
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{jm_algo}
  \caption{Normal JM imputation pseudocode}
\medskip
\small
Taken from van Buuren’s book “Flexible imputation of missing data” \cite{VanBuuren2012}
\end{figure}
There has been extensive research on using the normal model for this, and research shows that it even performs well under situations where the data has strong non-normality. An obvious issue arises when we have discrete or categorical data. There has been much debate in the literature about what to do with in this case. Some authors argue that you should just impute under a continuous distribution and round imputations to the nearest class number, and others suggest using distributions that are more suited for categorical data \cite{VanBuuren2012}.  !! cite here that it is inferior when categorical data present or later? !!There are a few R packages for joint modelling imputation include Amelia \cite{Honaker2015}, norm \cite{norm2015} and cat \cite{cat2015}.

It is my opinion that unless we are very confident in the multivariate joint distribution, that JM should not be used. In our cancer example, we have many categorical variables and strictly positive variables to impute, so JM seems inappropriate

On the other hand, there is fully conditional specification (FCS). In this paradigm, missing data is imputed on a variable by variable case (on the columns), based off of a specification of the imputation model for each imputed variable (known as a “chained equation”).  These full conditionals should factor to specify the joint distribution. In the JM setting, we must give a k dimensional model, however in the FCS setting, we must give k one dimensional models. We are trying to sample from
$$P(Y,X,R|\theta)$$
By sampling from the full conditionals
$$P(Y_j|X,Y_{-j},R,\phi_j)$$
In this notation, $Y_-j$ means all of the columns with missing data except for j, and X is the fully observed columns (which could possibly be empty). A pseudocode example can be seen here
\begin{figure}[h!]
  \centering
    \includegraphics[width=0.8\textwidth]{fcs_algo}
  \caption{mice FCS imputation pseudocode}
\medskip
\small
Taken from van Buuren’s book “Flexible imputation of missing data” \cite{VanBuuren2012}
\end{figure}

One of the major flaws of this method is that in order for there to be a guarantee that we are sampling from the correct distribution, we need to ensure that our full conditionals are compatible, i.e that they factor into the proper joint. This is very hard to check in practice, but studies have shown that even when the models are highly incompatible, FCS methods are very robust \cite{VanBuuren2006}. But despite this, FCS allows us much more flexibility than JM does.

We are going to have to specify something, there is no escaping that, but I think that it is easier for the average person (especially a clinician) to be able to define a single distribution and model rather than to guess at a multivariate (which could have a really high dimension). In addition, in the survival analysis setting, we will naturally have time variables be only positive, and some binary indicators, whereas others can take any value. Trying to fit a parametric distribution with these stipulations will be very hard if not impossible, so we will be relegated to using a general distribution (like the normal), which will certainly elicit a poor fit. So, the fully conditional specification will be our choice.  In an ideal world, we would have complete data, and would not need to resort to imputation. But since we don’t have complete data, we must choose one method and accept its strengths and weaknesses.

Now that we have chosen the paradigm, we need to select an implementation of it. Many exist (such as MICE \cite{VanBuuren2011}, mi \cite{Su2011}, etc.). I wanted to select the implementation that combined ease of use, understanding, and programming. What I decided upon was a method called MICE- Multiple Imputation by chained equations \cite{VanBuuren2011} MICE is an FCS MCMC method that under compatibility, is a Gibbs sampler, where we obtain samples from the joint by sampling from the full conditionals. The user defines the full conditionals, so it is possible that the joint may only exist implicitly, and not actually have a functional form. 

In order to use mice, we must have that the missingness in our data to be MCAR or MAR. It can work with MNAR data, but it requires some extra modelling assumptions. This is a seldom observed case in practice, so the interested reader may check \cite{VanBuuren2011}  section 6.2 for a detailed look at this.  Enders proposes using t tests to test if the data is MAR or MCAR, but this is of little use for us, because we only want to know if the data is MNAR, which is impossible to test since testing for MNAR would entail us using information that is impossible to have \cite{Enders2010}. Luckily, we can safely assume MAR if there is reason to believe that some of the covariates collected account for the missingness. **cite?
%start here

It should be noted that in the real data we will use, the response variable is fully observed, but the covariates have a lot of missingness. If it were the case that we had missingness in the survival time, then the methods described above might not work. They might fail because the unobserved times may follow a different distribution than the observed times. This is cleared up by Zhao et. al in 2014 through Kaplan-Meier MI  \cite{Zhao2014}. This is beyond the scope of this report though so I omit its details. 

Once we have the correct assumptions, we need to set up our full conditionals imputation models. This may take a while for large datasets, but the extra time spent will ensure a better model. We choose what predictors will go into imputation, and what method to use (regression, predictive mean matching, logistic regression, etc.). We should choose predictor variables that are somewhat correlated with the missing data, as well as include the covariates that we are doing inference on, as to avoid bias. For variables that are derived from others, we impute the others and then compute that variable, in a process known as passive imputation. Since our data is manageable, I include any reasonable predictor that doesn’t induce collinearity for predictors. With large datasets, we may need to perform variable selection.  Since mice is an iterative process, we must choose how many iterations we will do until convergence. The older literature suggests only 5 is enough , but with modern computation, we can easily exceed this, even with large data \cite{VanBuuren2012}. A good way to assess how many iterations to use is to look at the diagnostics (which we will talk about later). Once the chains seem to converge, we can use that many iterations. As well, we need to decide how many datasets to impute. The early literature argued that 5 will due, but more is better, since it will cut down on simulation error. With the speed of computers and availability of storage, many authors now suggest using more imputations.

We need to verify that our imputations are valid once we complete them. First, we need to see if our chains have converged. Since mice is an MCMC method, we should check the chain for irreducibility, aperiodicity, and recurrence. To determine when the chain has converged, van Buuren suggests that“Convergence is diagnosed when the variance between different sequences is no larger than the variance within each individual sequence” \cite{VanBuuren2012}.  There is not much in the imputation literature about statistical tests to check for convergence, but a popular test is Gelman and Rubin’s rhat test. 

Once we have assessed convergence, we need to actually check that the values imputed are valid and come from the correct posterior. The overarching idea that we need to pay attention to is “does the data look like it could have been real data”. We can assess this in many ways, including density plots, box and whisker plots, etc. This is a visual task, and there is no statistical method to validate this.

This whole process can be very time consuming, because every time we want to make a change in the methods used, we have to rerun the algorithm and reassess our results. But once we find the setup that works for us, we don’t need to repeat it again. So, while it may take a lot of time now, setting up a proper model will save us even more time in the future.
%start here
Once we have $m$ imputed datasets, we may run any valid analysis (regression, computing any statistic) on each imputed dataset INDIVIDUALLY, treating each of the m datasets as if it was complete. We may then use Rubin’s rules \cite{Rubin1987} to pool our estimates. This will give us a point estimate, as well as the proper variance for the quantity we have in mind.  Rubin’s rules are essential for using multiply imputed datasets, so we need to investigate them thoroughly.

Rubin’s rules are a set of rules that guide us in making inference from multiply imputed data. It involves three parts. The first is getting an estimate of the population estimand Q, we do so by taking the average of the MI sample estimands ($\hat{Q}_i$) to get the MI estimate $\bar{Q}$.
$$\bar{Q}=\frac{1}{m}\sum_{i=1}^{m}\hat{Q}_i$$,
Where $\hat{Q}$ is the estimand evaluated from the data in the $i^{th}$ dataset.
The estimates are not set, and there is variance associated with them. The first form of variance is the “within” variance, or the variance or each estimate. We can get an MI estimate of this quantity by doing
$$\bar{U}=\frac{1}{m}\sum_{i=1}^{m}\bar{U}_i$$
Where $\bar{U}_i$ is the $i^{th}$ datasets variance
The other form of the variance is the “between datasets” variance. This is the variance associated with the fact that we have missing data. It is given by
$$B=\frac{1}{m-1}\sum_{i=1}^{m}(\hat{Q}_i-\bar{Q})$$
The total variance for our estimand is given by 

$$T=\bar{U}+B +\frac{B}{M}$$
The last term is our simulation variance, and its existence is proven by Rubin in \cite{Rubin1987}.
The theory is rooted in the assumption that whatever we are trying to pool is asymptotically normally distributed with mean Q and variance U. We don’t have these population values, so we must use what we have from the sample, namely $\bar{Q}$ and $\bar{U}$. With this assumption, we know that
$$\frac{Q-\bar{Q}}{\sqrt{T}}\sim t_{\nu}$$ 
And the degrees of freedom is shown to be \cite{Barnard1999}
$$\nu=\frac{\nu_{old}\nu_{obs}}{\nu_{old}+\nu_{obs}}$$
Where $\nu_{obs}=\frac{\nu_{com}+1}{\nu_{com}+3}\nu_{com}(1-\frac{B + B/m}{T})$
And $\nu_{old}=\frac{m-1}{(\frac{B + B/m}{T})^2}$
Rubin’s rules assume normality, so if our statistic in mind is not asymptotically normal, we need to transform it towards normality before we pool. There have been some research about how to pool non normal quantities, but current research shows poor results and power when doing so \cite{Marshall2009} . It should also be noted that we have discussed the univariate case, but this easily extends to the multivariate case. We now have a powerful framework to get valid inference from multiply imputed data, so let us actually work with it.

\section{Survival analysis}
\label{sec:Surv}
Now that we have the multiple imputation datasets created, we may run our analysis. As a general rule of thumb, we should run our desired analyses on the available data first, to get an idea of what to expect. Because we are working with cancer data, we are interested in some basic survival quantities (Kaplan-Meier survival estimates for survival function, log rank test to test for similarity of curves, Cox regression to determine hazard ratio). Following Rubin’s rules, we run the individual analyses on each of the $m$ datasets, and then pool our results. In this section we will discuss each type of survival analysis in the MI setting and the issues associated with it.

Before we begin, it should be noted that there is another way to work with the multiply imputed data. This method is colloquially called the “stack method”. For the stack method, we take all of our imputed data and “stack” them one on top of each other to get one huge dataset of size $(m*i) x j$. Under the stacked method, we can produce unbiased estimates of quantities of interest, but the estimates of variance will be too small (since we are artificially increasing the sample size). Thus, the stacked method is a poor choice for running any hypothesis tests or quantifying uncertainty. It is not useless though. The stacked method is useful when we want to analyze just one plot instead of $m$ for model checking. As well, the stacked method may be useful in situations where we partition categorical data on an imputed variable and then look at the percentage in each category. Under Rubin’s rules, we are not guaranteed that the percentages will sum to unity, but under the stacked method we are.

Analysis for the Kaplan-Meier estimate is quite simply in the non MI setting, but special care should be taken in the MI setting. First, we need to do is clearly define what the groups and population are, and what constitutes an event of interest. A very common mistake that researchers make is to try to frame a competing risks problem as a Kaplan-Meier problem. We also need to be sure that we have noninformative censoring (given the covariates, censoring is independent of the event). Once we have checked all of these, we can run the Kaplan-Meier curve on each of the $m$ datasets. Now, we could pool these estimates, but that would be ill advised, because the Kaplan-Meier curve is not normally distributed. To get around this, it has been proposed by Marshall et. al to take the complimentary log log transformation of the survival estimates before pooling \cite{Marshall2009}. We can make this transformation, pool our results, and then back transform to get the pooled KM estimate. 

In the MI setting, an interesting situation may arise when the last event (and thus the range of survival time) differs between the imputed datasets. This is the result of a person with a long survival time being put in different groups via imputation. We can deal with this by either extending the last observed Kaplan-Meier estimate out until the last event time, or truncating all of the imputed curves at the minimum time of last event. In traditional analysis, we would not extend out the Kaplan-Meier curve out past the last event, but it seems acceptable to do in the MI case (so that we can make inference and are not hampered by one poor imputed dataset).
%start here
One of the main tasks that clinicians are interested in is the median survival time (the smallest time that the survival function is less than .5), specifically, the variance at the median. The median is a much better estimate of the typical survival time than the mean is, because in survival analysis, the time to event is typically right skewed, so the mean survival time is almost certainly not relevant to the typical patient/user of the km plot. We could get an unbiased estimate of the median via the stacked method, but this will give us false confidence, because our sample size is greatly inflated. Having a confidence interval for the median is actually very important, so we will need a method that allows us to do this.  We can go about it in two ways. The first way is to pool the greenwood variance associated with each time point and then take the average as the variance at that time point, but this would lead to too big of changes between the data sets (by construction of the greenwood estimator). The solution for this issue is to derive the variance of the median by the “reflection method”. In this method, we first fit the MI Kaplan Meier curve, and then construct a 95\% confidence interval for all time points with the total variance obtained from Rubin’s rules pooling. The median is defined as the first time when the pooled MI curve crosses .5 survival line, and the lower and upper bounds are the points where the lower and upper bands cross the .5 survival line respectively. 


Now, we will have a pooled estimate of the true survival curve. In the typical setting, we might want to look to see if these curves are similar to each other. We would do this with a log-rank test under the regular setting. However, we should not be deceived. We have an averaged survival curve. It is not constructed in the same way that a regular Kaplan-Meier curve is, so we cannot get the quantities that we would need to compute the log rank test. However, we can still get the pooled log rank test. To do so, we can do one of two things. The first is to run the logrank test on each of the datasets and then pool via Rubin’s rules. This is the logical way to do it, but the log rank statistic is not normally distributed, and no obvious transformation comes to mind. Another option is to run a cox regression on just the group in question. From the cox regression, we can obtain the score test, which is in fact the log rank test, but that again is chi square distributed. We know that the Wald test is asymptotically equivalent to the score test, so we can use the Wald test of the coefficient as a proxy for the log rank test. In this way, we get a quantity that is normally distributed, so we may use Rules.

We would now like to model the hazard ratio via the Cox proportional hazards model. The overall goal will be to fit a Cox model with baseline covariates, check to see if it passes the proportional hazards assumption, and then add in the treatment variables to see how they affect the hazard. It is known that the cox regression coefficients are normally distributed, so there is no issue in pooling, but we do need to be careful about checking the proportional hazards assumption !! source or known?!! The very first thing that we need to do is check to check the available case model to assess if we have proportional hazards. If one of the covariates truly is dependent on time, adding imputed data isn’t going to change that, so checking the available case analysis is a good sanity check. The way we go about checking to make sure that we have proportional hazards is checking to see if the schoenfeld residuals are correlated over time for each covariate. We can check a test for correlation or observe a spline fit to the residuals. In cancer research, the most common test to look for proportional hazards is to plot the spline fit to the residuals along with the 95\% confidence intervals, and see if any straight line could pass through the bounds. There isn’t an official name for this method, but the straight edge method seems to be a fitting name.  If this is the case, then we say that that the covariate in question follows the proportional hazards assumption.

We can take our imputed data and fit a cox model on each of the $m$ datasets, and pool them easily. But how is the best way to check the proportional hazards assumption. We can go about this in a few different ways. The first is to check the assumptions on each individual model fit to each dataset. This may prove to be an arduous task, but with graphical tools such as shiny, this isn’t too bad. We could also superimpose all of the spline fits on one plot, and see how the shape and general trend compare to the available case analysis. We can also use the stack data to get just one set of plots, but the straight edge method will not work here since the errors are too low. Rather, we would just need to assess the shape of the spline fit in comparison to the available case method. Once we have verified that the model follows the proportional hazard assumption, we may trust its results. We can now add in our treatment covariates, and see how they affect the hazards.
The last thing that we might be interested in is the cause specific hazard, and the cumulative incidence function. Work on this a lot, and decide if I even want to put this in.

\section{Propensity Score Analysis}
\label{sec:Propscore}
Now that we have laid down the theory for analyzing the survival section for clinical relevance, we can move on to the causal analysis part. While there is a lot of preparatory work that goes into the theory of it, the results that can be obtained using causal analysis framework and propensity scores is much stronger than conventional analysis.  As well, with causal analysis, we get a cause and effect result, which is in tune with what the general population believes that results should be. Propensity score methods are an easy to understand yet powerful tool. The use of propensity scores justified in Rosenbaum and Rubin’s 1983 paper \cite{Rosenbaum1983}.   Our overall goal is to estimate the average treatment effect in a setting where the initial study was not a completely randomized experiment. Propensity score analysis helps us to do this by balancing the groups out so that it becomes more like a randomized controlled experiment.

We will need to make a few decisions along the way. Our very first decision comes when deciding how to use the propensity score. There are four main uses in the survival literature: Matching, stratification, weighting, and covariate adjustment \cite{Austin2014}. The goal is to balance the treatment and control groups, such that the only difference between the groups is due to the treatment received, and not any underlying factor. All of these methods will help us to examine the average causal effect, but each goes about it in a different way. Propensity score matching is the easiest to understand, yet it is still quite powerful, so it will be the method that we will use

Our next decision comes as to how to use matching in the multiple imputation setting. The stacking method described before would obviously be inappropriate, as we would have spurious and repetitive matches due the falsely inflated sample size.  Matching on the stacked set would give us much more power to detect a difference, but the results from it would not be valid.

There is hope though for matching with multiple imputation data. Two methods are described in Mitra and Reiter about how to do this \cite{Mitra2012}. In the first method, propensity score matching is done within each MI dataset (known as within matching). This, we will get $m$ estimates of the average treatment effect to which we will average. The other method, known as the across method takes the average propensity score for each individual among the $m$ imputed data sets, matches on the averaged propensity scores, and estimates the treatment effect in that manner. Both methods have their pros and cons, and are appropriate for different scenarios. However, as we will see in the applied example, the treatment variable may itself have missingness, and thus needs to be imputed. In this situation averaging across datasets does not make sense (since there is no guarantee that a given subject in imputation $i$ has the same treatment in imputation $j$), so we must use the within method. 
%does the fact that the drug is an imputed value effect which model we should use? The problem exists %that the treatment is unknown in some, so it needs to be imputed.  Thus, we could have 30 datasets %where subject I is treatment, and 20 where he is a control. We can either classify as treatment via %majority voting. If not this, then we need to change to the within method.
Propensity scoring using logistic regression is the most widely used method, because it is computationally simple and is well understood by both clinicians and statisticians. Thus, we will use it to compute our propensity scores. Different propensity scores will be obtained according to what predictors we use in our model, so we need to be sure that we fit a model with clinically relevant and meaningful predictors. Our overarching goal is to account for any covariate that leads to the selection of the treatment.There has been significant debate among statisticians about how to set up these models, by either throwing in every possible variable into it, or only include ones known to affect treatment selection. I don’t plan to settle this debate, but since we are in a setting where simplicity is a goal, I plan to use only pretreatment covariates deemed to be important (through consultation with subject matter experts) to treatment selection. This is the method that Peter Austin (a well-known researcher of propensity score analysis) recommends \cite{Austin2014}.

There exists research on how to test if we have properly set up the propensity score model (do this if mao sends the code). It is not clear how this could be applied in the multiple imputation setting, but I think that if used on the available cases, it would give us a good indication that we are heading in the correct direction.

The matching can be done in many different ways like a nearest neighbor, caliper, or mahalanobis distance matching. It shouldn’t really matter which one we use, so long as the groups are similar in distribution after matching in each dataset.
Once an acceptable propensity score model is selected, we will run it on each of the $m$ imputed datasets, match within each dataset, obtain the average causal effect, and then pool. We will also be interested in the reduction of the probability of death. To observe this, we will need to check it in each dataset and then pool. 

Now we would like to put the propensity scores into our cox model. Doing so will allow us to assess the hazard ratio of covariates while controlling for group imbalances. Thus, the results ought to mimic the situation where the groups were chosen by random assignment. Although it has been shown that the results from using it in the cox model are biased, it can still be used as a useful explanatory tool \cite{Austin2014} .We are still in the multiple imputation framework, so what we will need to do is to fit our models on each dataset, get the results, and then pool them via Rubin’s rules.

