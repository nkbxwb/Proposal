\chapter{Methods}
\section{Multiple Imputation}
\label{sec:MI}
I want the framework we use to be easy to use and understand, so that it can easily be discussed among clinicians and other non statistically minded people. On the same token, I want the methods and theory to be sound from a statistical point of view. For the three parts that we are combining, I try to pick the optimal theory and software packages to achieve this. 
Our first decision comes as to what paradigm we should impute under. It should be noted that as long as we can produce valid imputations, the choice of method does not matter. However, since the base of our analysis starts with imputation, we need to make sure that we pick a good method. Everything that follows in the analysis is dependent on our imputed data, so it is necessarily the case that bad imputation will lead to poor results (be it bias, high variability, loss in statistical power).
 There are two main divisions in multiple imputation, and they are joint modelling and full conditional specification. In joint modelling, we assume that the data can be described by a multivariate distribution (specified by the user), and draw imputations from a joint distribution of the unknowns for the rows, given what we do know and their associated hyperparameters. There has been extensive research on using the normal model for this, and research shows that it even performs well under non-normality. R packages for joint modelling imputation include Amelia (source), NORM, and cat (Schafer source)
On the other hand, there is fully conditional specification. In this paradigm, missing data is imputed on a variable by variable case on the columns, based off of a specification of the imputation model for each imputed variable. These full conditionals should factor to specify the joint distribution.
We are going to have to specify something, there is no escaping that, but I think that it is easier for the average person (especially a clinician) to be able to define a single distribution rather than to guess at a multivariate. In addition, in the survival analysis setting, we will naturally have time variables be only positive, whereas others can take any value. Trying to fit a parametric distribution with these stipulations will be very hard, so we will be relegated to using a general distribution (like the normal), which will certainly elicit a poor fit. So, the fully conditional specification will be our choice.
Now that we have choose the paradigm, we need to select an implementation of it. Many exist (such as MICE, mi, etc.ect). I wanted to select the implementation that combined ease of use, understanding, and programming. What I decided upon was a method called MICE- Multiple imputation via chained equations. (SOURCE). MICE is a MCMC method that under compatibility, is a gibbs sampler, where we obtain samples from the joint by sampling from the full conditionals. Compatibility means that knowing the full conditionals allows you to know the joint. Specifically <<See 4.5.4 in van burren>> The user defines the full conditionals, so it is possible that the joint may only exist implicitly, and not actually have a functional form. This is an obvious disadvantage of this method, but multiple studies have shown that MICE is robust to this flaw, even when using highly incompatible models<<sources for this>>. Mice has been shown in simulation studies to yield unbiased estimates with proper coverage.
In order to use mice, we must have that the missingness in our data is missing at random (MAR). What this means is that the probability of a covariate being missing is related to another covariate that we have collected (for example, obese teens may not want to volunteer their weight, but knowing their sex and age will alert us to this). The opposite of this is MNAR- missing not at random. This is when the probability of a covariate being missing is dependent on another covariate that we don’t have, for example, if obese male teens don’t want to volunteer their weight, but we don’t collect age or gender. MICE will work under both MAR and MNAR, but performs much better under MAR. There are tests for testing ??, but they aren’t very widely used)
Once we have the correct assumptions, we need to set up our full conditionals. This may take a while for large datasets, but the extra time spent will ensure a better model. We choose what predictors will go into imputation, and what method to use (regression, predictive mean matching, logistic regression, etc.). For variables that are derived from others, we impute the others and then compute that variable, in a process known as passive imputation.  Since mice is an iterative process, we must choose how many iterations we will do until convergence. The literature suggests only 5 is enough (source), but with modern computation, we can easily exceed this, even with large data. As well, we need to decide how many datasets to impute. Once again, 5 will due, but more is better, since it will cut down on simulation error (find where I wrote up the reasons why).
We need to verify that our imputations are valid once we complete them. The overarching idea that we need to pay attention to is “does the data look like it could have been real data”. We can assess this in many ways, including density plots, box and whisker plots, etc.
Once we have m imputed datasets, we may run any valid analysis (regression, computing any statistic) on each imputed dataset INDIVIDUALLY. We may then use rubins rules (source) [put the steps in later], to pool our estimates. This will give us a point estimate, as well as proper variance for the quantity we have in mind. Rubins rules assumes normality, so if our statistic in mind is not asymptotically normal, we need to transform it towards normality before we pool.

\section{Survival analysis}
\label{sec:Surv}
Now that we have the multiple imputation datasets created, we may run our analysis. The analyses, as stated in the previous section are heavily based on Rubin’s pooling rules. We are interested in some basic survival quantities (Kaplan-Meier survival estimates for certain groups, cox regression to determine hazard ratio), as well as some more advanced ones (survival in the competing risks setting). Following rubins rules, we run the individual analyses on each of the m datasets, and then pool our results. This works fine for the cox model, as the regression coefficients are asymptotically normally distributed. In many of the MI packages, there are functions to model and pool with cox regression. Care needs to be taken to ensure that our analysis is valid (ie that we do in fact have proportional hazards). We can go about this in two ways. The first is to check the assumptions on each individual model fit to each dataset. This may prove to be an ardous task, but with graphical tools such as shiny, this isn’t too bad. We may also “stack” our multiple imputations on top of eachother, and run one huge model (thus our MI data are acting as replicates). This will produce unbiased estimates, but the standard errors will be too low. So, this tool may be used as a graphical check, but the results from the inference of this cannot be trusted(source for all of the above). Once we have verified that the model follows the assumptions, we may trust its results, and perform inference on the parameters using the obtained total variance.

