\chapter{Methods}
I want the framework and methodology we use to be easy to use and understand, so that it can easily be discussed among clinicians and other people who don’t have a statistics or mathematics background. On the same token, I want the methods and theory to be sound from a statistical point of view. For the three parts that we are combining, there are a lot of differing theories and paradigms. I aim to pick the ones that optimize ease of understanding and power of results. Along the way, we will also develop new methods and validation tools, as well as apply the existing theory to situations that it has not previously been applied to.
\section{Multiple Imputation}
\label{sec:MI}
Our first decision comes as to what paradigm we should impute under. It should be noted that as long as we can produce valid imputations, the choice of method does not matter. However, since the base of our analysis starts with imputation, we need to make sure that we pick a good method. Everything that follows in the analysis is dependent on our imputed data, so it is necessarily the case that bad imputation will lead to poor results (be it bias, high variability, loss in statistical power, etc.). The methods we will discuss here are geared towards and motivated by cancer research, but can be easily adapted to other areas.

There are two main divisions in modern multiple imputation, and they are joint modelling and full conditional specification. Both have their own flaws and advantages. I will describe both, and then explain why full conditional specification is better suited for cancer research.

Before we get in to the imputation models, we need to have a firm understanding of missing data concepts. They take up quite a bit of space to explain, but they are fundamental concepts. If you are unfamiliar with them, please read appendix \ref{app:apdx}  before reading further.

In joint modelling (JM), we assume that the missing data mechanism is ignorable and that the data can be described by a multivariate distribution on the rows of the data (specified by the user). We then draw imputations from the joint distribution of the unknowns for the rows, given what we do know and their associated unknown parameter of the imputation model. !!!!Example here!! Since we don’t know the true model parameters, we need to estimate them. This is often done by a data augmentation algorithm \cite{VanBuuren2012}. There has been extensive research on using the normal model for this, and research shows that it even performs well under data that has strong non-normality. An obvious issue arises when we have discrete or categorical data. There has been much debate in the literature about what to do with it. Some authors argue that you should just impute under a continuous distribution and round, and others suggest using distributions that are more suited for categorical data \cite{VanBuuren2012}. There are a few R packages for joint modelling imputation include Amelia \cite{Honaker2015}, norm \cite{norm2015} and cat \cite{cat2015}.
On the other hand, there is fully conditional specification (FCS). In this paradigm, missing data is imputed on a variable by variable case (on the columns), based off of a specification of the imputation model for each imputed variable. These full conditionals should factor to specify the joint distribution. In the JM setting, we must give a k dimensional model, however in the FCS setting, we must give k one dimensional models. We are trying to sample from
$$P(Y,X,R|\theta)$$
By sampling from the full conditionals
$$P(Y_j|X,Y_{-j},R,\phi_j)$$
In this notation, $Y_-j$ means all of the columns with missing data except for j, and X is the fully observed columns (which could possibly be empty). 
We are going to have to specify something, there is no escaping that, but I think that it is easier for the average person (especially a clinician) to be able to define a single distribution rather than to guess at a multivariate. In addition, in the survival analysis setting, we will naturally have time variables be only positive, whereas others can take any value. Trying to fit a parametric distribution with these stipulations will be very hard, so we will be relegated to using a general distribution (like the normal), which will certainly elicit a poor fit. So, the fully conditional specification will be our choice.
Now that we have chosen the paradigm, we need to select an implementation of it. Many exist (such as MICE, mi, etc.). I wanted to select the implementation that combined ease of use, understanding, and programming. What I decided upon was a method called MICE- Multiple imputation via chained equations. (SOURCE). MICE is a MCMC method that under compatibility, is a Gibbs sampler, where we obtain samples from the joint by sampling from the full conditionals. Compatibility means that knowing the full conditionals allows you to know the joint. Specifically <<See 4.5.4 in van Burren>>  \cite{VanBuuren2012}The user defines the full conditionals, so it is possible that the joint may only exist implicitly, and not actually have a functional form. This is an obvious disadvantage of this method, but multiple studies have shown that MICE is robust to this flaw, even when using highly incompatible models<<sources for this>>. Mice has been shown in simulation studies to yield unbiased estimates with proper coverage.
In order to use mice, we must have that the missingness in our data is missing at random (MAR). What this means is that the probability of a covariate being missing is related to another covariate that we have collected (for example, obese teens may not want to volunteer their weight, but knowing their sex and age will alert us to this). The opposite of this is MNAR- missing not at random. This is when the probability of a covariate being missing is dependent on another covariate that we don’t have, for example, if obese male teens don’t want to volunteer their weight, but we don’t collect age or gender. MICE will work under both MAR and MNAR, but performs much better under MAR. There are tests for testing ??, but they aren’t very widely used)
Once we have the correct assumptions, we need to set up our full conditionals. This may take a while for large datasets, but the extra time spent will ensure a better model. We choose what predictors will go into imputation, and what method to use (regression, predictive mean matching, logistic regression, etc.). For variables that are derived from others, we impute the others and then compute that variable, in a process known as passive imputation.  Since mice is an iterative process, we must choose how many iterations we will do until convergence. The literature suggests only 5 is enough (source), but with modern computation, we can easily exceed this, even with large data. As well, we need to decide how many datasets to impute. Once again, 5 will due, but more is better, since it will cut down on simulation error (find where I wrote up the reasons why).
We need to verify that our imputations are valid once we complete them. The overarching idea that we need to pay attention to is “does the data look like it could have been real data”. We can assess this in many ways, including density plots, box and whisker plots, etc.
Once we have m imputed datasets, we may run any valid analysis (regression, computing any statistic) on each imputed dataset INDIVIDUALLY. We may then use Rubin’s rules (source) [put the steps in later], to pool our estimates. This will give us a point estimate, as well as proper variance for the quantity we have in mind. Rubin’s rules assumes normality, so if our statistic in mind is not asymptotically normal, we need to transform it towards normality before we pool.

\section{Survival analysis}
\label{sec:Surv}
Now that we have the multiple imputation datasets created, we may run our analysis. The analyses, as stated in the previous section are heavily based on Rubin’s pooling rules. We are interested in some basic survival quantities (Kaplan-Meier survival estimates for certain groups, cox regression to determine hazard ratio), as well as some more advanced ones (survival in the competing risks setting). Following Rubin’s rules, we run the individual analyses on each of the m datasets, and then pool our results. This works fine for the cox model, as the regression coefficients are asymptotically normally distributed. In many of the MI packages, there are functions to model and pool with cox regression. Care needs to be taken to ensure that our analysis is valid (i.e. that we do in fact have proportional hazards). We can go about this in two ways. The first is to check the assumptions on each individual model fit to each dataset. This may prove to be an arduous task, but with graphical tools such as shiny, this isn’t too bad. We may also “stack” our multiple imputations on top of each other, and run one huge model (thus our MI data are acting as replicates). This will produce unbiased estimates, but the standard errors will be too low. So, this tool may be used as a graphical check, but the results from the inference of this cannot be trusted(source for all of the above). Once we have verified that the model follows the assumptions, we may trust its results, and perform inference on the parameters using the obtained total variance.
We now would like to calculate the Kaplan-Meier curve for a subset of the population.  While this is a pretty simple task in the complete case setting, it is a little more difficult in the multiple imputation setting. One of the main tasks that clinicians are interested in is the median survival time (the smallest time that the survival function is less than .5), specifically, the variance at the median. The median is a much better estimate of survival than the mean is, because in survival analysis, the time to event is typically right skewed, so the mean survival time is almost certainly not relevant to the typical patient/user of the km plot. We could get an unbiased estimate of the median via stacking the imputations and then running a km model, but this will give us false confidence, because our sample size is greatly inflated.  The correct way to go about this is to run a Kaplan Meier curve on each of the imputed datasets, and then pool the estimates. We will run into two problems here though. The first is that the Kaplan Meier curve is not normally distributed, so we will need to pretransform the data before pooling towards normality. [source] claims the complimentary log log transformation helps with this, and this transformation was implemented in [prostate] paper  successfully.  The second issue is that the variance at a point is usually computed under greenwoods formula, which depends on the order of the data. We can stack, but this will lead to too small of an estimate. As well, we could pool on greenwoods formula, but this would lead to too big of changes between the data sets (by construction of the greenwood estimator). The solution for this issue is to derive the variance of the median by the “reflection method”, is look at all of our Kaplan Meier curves, and take the median to be the average median, and take the confidence interval to be the time the first and last curve cross the 50 line * *work on this*. 
The last thing that we might be interested in is the cause specific hazard, and the cumulative incidence function. Work on this a lot.

\section{Propensity Score Analysis}
\label{sec:Propscore}
Now that we have laid down the theory for analyzing the survival section for clinical relevance, we can move on to the causal analysis part. While there is a lot of preparatory work that goes into the theory of it, the results that can be obtained using causal analysis and propensity scores is much stronger than conventional analysis.  As well, with causal analysis, we get a cause and effect result, which is in tune with what the general population believes that results should be. Propensity score methods are an easy to understand yet powerful tool. Our overall goal is to estimate the average treatment effect in a setting where the initial study was not a completely randomized experiment.
We will need to make a few decisions along the way. Our very first decision comes when deciding how to use the propensity score. We can either choose to match or stratify on the propensity score. The use of doing either is justified in Rubin’s paper !!Rubin 1983!!. Both will help us, but matching is easier for the layperson to understand, and easier to implement, so we shall use that.
Our next decision comes as to how to use matching in the multiple imputation setting. The stacking method described before would obviously be inappropriate, as complete records would always be matched to themselves, and imputed values would often be matched with their selves from another imputation. As well, we would have a falsely inflated data, giving us confidence in our matches where we should not have any.
Two methods are described in Robin and Mitra !!sauce!! about how to do this. In the first method, propensity score matching is done within each MI dataset (known as within matching). This, we will get m estimates of the treatment effect to which we will average. The other method, known as the across method takes the average propensity score for each individual and estimates the treatment effect in that manner. Both methods have their pros and cons, but Mitra and Reiter show that the across method limits bias more than the within one does. I need to determine what to use, because I might want to use inverse PS weighting in the cox model.
