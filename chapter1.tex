\chapter{Introduction}
\section{Motivation}
\label{sec:Motivation}
The motivation of this thesis is to develop methodology that can be used both by applied researchers and clinicians to draw meaningful survival inference from data with a high amount of missingness. 
I want the methods  to be easy enough to describe to someone with a limited statistical background, 
but meaningful and valid so that the results obtained can be used in publication. 
The desire to have it this way stems from working on a related project with both statisticians and clinicians.

\label{ch:Intro}
Missing data is a major problem in both applied and theoretical statistics, however, 
it has not received attention proportional to its need. Survival analysis is well studied, 
but is relatively complete, so not much new research comes out of this field. 
Propensity score analysis will help us determine causal relationships when we don’t have a completely
randomized experiment. As one could imagine, all three of these fields are important to the applied statistician,
as they will come across at least one at some point in their career.  The goal of this thesis is to demonstrate how to use all three in trio, a topic that has only received little interest in the literature.
I will explain each of these three disciplines in detail before we dive into combining them.

\section{Imputation}
\label{sec:Imputation}
Imputation (specifically multiple imputation) is a way to “fill in missing data” with plausible values, and it forms the base of this paper. All of the other analyses that will be used will follow from it, thus we need a good understanding of it before we may proceed. Imputation itself has been around for some time, but multiple imputation is a recent development, proposed formally in 1987 by Donald Rubin \cite{Rubin1987}.


At first, statisticians payed no attention to missing data, and happily discarded records for their analysis that were incomplete. This procedure is  known as complete case analysis. There are many problems with this paradigm. To begin with, you will lose a lot of statistical power when doing this, because you are literally throwing away records and thus decreasing your sample size.  In addition, this can be costly to the researcher. If it costs a set amount to collect a single record, and you don’t use this record, you are literally wasting money. As well, in some rare cases, incomplete data might be the only type we can get. Lastly, and most importantly, we will be biasing our estimates if we discard them. For example, if we have a random sample of people and are testing a drug, and want to run a regression on some collected covariates. Men are known to not want to give all of their information, so they leave them blank. In the analysis, we will need to discard the male samples because they are incomplete, leaving us only with women. Thus, we don’t have a random sample anymore, and will get biased results.

A slight improvement on this is called available case analysis. In this setting, a record is used if it has all of the needed information for an analysis. So, a record could have missingness, but if the covariate with missingness is never used in the analysis, it will not be discarded. This is the standard for most statistical packages. It is better than complete case analysis, but is still flawed. We are still throwing away valuable data, and available case analysis will still lead to bias, nonsensical situations (like correlations outside of $\pm 1$, and inconsistent sample numbers for different analyses.

The next wave of statisticians wanted to improve upon this, so they developed what we now call today imputation. Their specific incarnation was called single imputation, and their goal was to fill in missing values with a plausible replacement value. In single method (such as regression, mean, trees) is used one time to impute the missing value. While this is a little better than complete case analysis, it still has many drawbacks. Asserting that a single value is the true value is unjustified and foolish. There is always some amount of error involved, and we can in no way be 100\% confident that our imputed value is correct. Furthermore, if I impute one value and you impute another, we may get totally different results from analysis on the data. This is obviously not desirable.  In addition, imputing one time and calling it your data will artificially increase your sample size. You are in effect treating the imputed values as if they were real. While single imputation certainly has its drawbacks, the idea of actually trying to fill in the data is an important one, and multiple imputation fills in the gaps that single imputation is not able to cover.

Multiple imputation began in the 1970’s, but it wasn’t until 1987 when the Donald Rubin proposed multiple imputation methodology did it start to gain acceptance \cite{Rubin1987}. The central idea is to produce many values to substitute in for the missing value, drawing these values from the missing covariates posterior distribution. Using these substitute values (m values), we can think of the data now as being m datasets, each dataset having the observed data, and one value of the missing data. Once we have a sufficient number of datasets, we can run whatever analyses we would like on them individually, and then pool the results. We can get the standard errors by noting the within and between imputation variance. This is obviously much better than the first two methods because it allows is to not throw away data, as well as allowing us to quantify our uncertainty about imputing the missing values. The only real drawback of multiple imputation is that we still don’t have true data, but we can be confident enough in our estimations to compensate for that. As well, the method for drawing from the posterior can be a topic of debate. Multiple imputation use has been steadily increasing over the past 30 years, and it is now the standard for missing data. Stef van Burren, an influential author in multiple imputation did a study of academic papers, and concluded that the number of publications using or mentioning multiple imputation is growing at an exponential rate since about 1990 \cite{VanBuuren2012}

%start here
\section{Survival}
\label{sec:Survival}
Survival analysis is a huge field, and there have been many textbooks written about it. I only plan to introduce the topics that are relevant to my case study.  For a much more detailed introduction, please see X Y Z
Survival analysis on the whole is analysis of time to event data, often in the presence of censoring.  The two main tools that we will be using are Kaplan-Meier estimate and Cox regression. The Kaplan-Meier estimate is a non-parametric estimate of the true survival function (the probability that you survive after a time t). Cox regression is a modelling tool that allows us to analyze the hazard ratio of a covariate. Using cox regression, we can make statements such as   “increasing the drug by one mg will increase its hazard by 30%”. The hazard is a function that gives us information about survival after time t, given that the person has survived until time t. We need much more work here

\section{Propensity Score Analysis}
\label{sec:PSA}
<<THIS NEEDS A LOT MORE WORK>>
In an ideal world we would like to be able to do research and say that A causes B, not something along the lines of “our study says that A is related to B”.  Using propensity score matching, we are able to get this interpretation, via Rubin’s causal model. Rubin’s causal model is a framework that allows us make causality statements, and propensity score is a tool that grants us access to the framework.
The way propensity score analysis works is that we will match patients on their “propensity” to be in the treatment group. This is often done by logistic regression, although newer methods include regression trees and such.
