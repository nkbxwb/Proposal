\chapter{Discussion}
Even though the analyses and interpretations in the survival and causal sections go about answering the question in different ways, the interpretations of them should be clear. For chemotherapeutic treatments, the analyses show that Capecitabine is not better than any other chemotherapeutic treatment, but any chemotherapeutic treatment is better than none. For HER2-directed therapies, it seems that there is no difference between Lapatinib and Trastuzumab, but that both are better than no HER2-directed therapy.

In the analysis, we often talk about ``other'' chemotherapeutic. However, this is not descriptive. In a future analysis, it would be wise to classify the other chemotherapeutic agents, so that analysis could be run to determine which drug is actually the best (in terms of hazard reduction or survival function change). Currently, all we know is that Capecitabine might not be the best chemotherapeutic drug, but we cannot say which is better. 

We have discussed a number of tools and methods to analyze survival data with missingness and make causal inference.  There are lots of decisions to be made along the way, and I am in no way advocating that my exact choices will be proper for all situations, I am only claiming that the decisions made were proper for the type of data and questions that we had. I hope that I have given the reader enough information to run their own analysis, even if they don't choose the options that I did.

There will certainly be many disagreements about the multiple imputation portion. And since the multiple imputation serves as the root of the analysis, the concerns should be addressed. The first concern comes from people who don't understand or believe in imputation of missing values. Multiple imputation is a tool to help us find plausible values for missing data. We make no claim that each of the imputed values are correct, but when used properly, the results from subsequent analyses using the MI data will be unbiased. We aren't using multiple imputation to create data where there is none, rather we are using it to ``fill gaps'' in places that we already do have data. We actually need to impute in certain cases if we want to get valid results, as analysis without imputation will lead to severely biased results \cite{VanBuuren2012}.
\begin{comment}
Might want to put this earlier on
 For example, if teenage males who are obese don't want to self-report their weight, then classic available case analysis will yield biased results because we have knowingly left out part of the population who are systematically different We need to impute to make sure we have included all of the information and not to bias our estimate.
\end{comment}

The next and more substantial critique will come from statisticians who may not believe that the distribution that the imputations are being drawn from is valid. No matter what method we use to impute, we have to make a parametric assumption, be it the joint model for JM or the full conditionals for FCS.  For our case, using the normal model is certainly wrong because we have so many categorical and strictly positive variables (which is proven to be suboptimal in \cite{Kropko2014}), so we are left only with using FCS. And FCS alone has weak theoretical justification. But as we have discussed before, many studies have shown that FCS is robust to non-compatibility. As well, there was no formal model validation (such as cross validation), only ad hoc checks. In the literature there is hardly any mention of validation, because if we were to cross validate, we would be drawing from different models, and comparison between the folds would be like comparing apples to oranges. As well, if we were to pick imputations based on a loss rule, we run the risk of picking imputations that minimize the loss, but are useless in practice \cite{VanBuuren2012}.

\begin{comment}
An interesting future extension to this project would be to use a non parametric approach to multiple imputation, such as the one suggested by Long et al in \cite{Long2012}. But at the time of publication, there is not much literature or software on this subject, so I felt that it was not appropriate to use its results.  
\end{comment}
To summarize about multiple imputation, I would say that it is a necessary evil. In the process of using multiple imputation, we lose predictive power, and are forced to use a distribution that may not fit the data to a t. But we need to use imputation techniques if we wish to make any sense of our data with missingness. The advice I would give to those who are hesitant to use multiple imputation would be to not have missing data, but this is a task that is easier said than done. Multiple imputation is becoming the standard for missing data techniques, especially in the medical field. There are lots of pros to it, but there are certainly some cons. Much research has already gone in to MI, but much more needs to be done.

Next we can critique the survival section. We made a lot of assumptions about how our subjects were censored. We assumed that all of our subjects who were censored were right censored and non-informative. This seems to be a valid assumption, but there certainly exists left truncation. It may have been the case that there were some left truncated subjects, but once we landmarked, we certainly incurred some left truncation. When the log rank tests were performed, we needed to use many proxies to actually get the test result. Although it is theoretically justified, intuitively, it is not easy to follow. A topic for future research would be how to optimally combine MI test statistics, because currently the literature on it is slim and the methods are poor.

We decided to use Kaplan-Meier and Cox analyses because they are very standard in practice, and answer the questions well. However, some other methods could have been used. A popular theoretical model is called the accelerated failure time model (AFT), which describes how covariates affect the survival time, assuming that it acts in a multiplicative fashion. As well, we only focused on death data, and did not take into consideration competing risks. Had the dataset been more robust, perhaps we could have incorporated competing risks.  
\begin{comment}
Next, since there is no well-established method to validate the model, we had to be creative and define our own methods to check them. The methods are reasonable and both the stacked and individual methods are very similar. More research should be done though to verify if this will always be the case, specifically in the presence of pathological data. 
We had some survival data in the dataset that would be appropriate for the competing risks setting, but this was not of primary concern to the clinicians. In future research, analyzing this data would be of interest since it poses many interesting problems.
\end{comment}

There are three concepts in survival analysis that I find interesting, but our data did not allow for it. The first is variable selection. The clinicians knew what they wanted to test, so this was not needed, but variable selection in the context of MI is an interesting question, and van Buuren covers it in his book \cite{VanBuuren2012}. This would be very useful if our dataset had covariates that we were unsure of their predictive power or wanted to examine. 

Another interesting addition would be using multistate data. In this setting, subjects can transfer from one group to another, i.e. have cancer, metastasize to the brain, go in to remission, and then relapse.  We model the states as a stochastic process. This would be really interesting, and I would have liked to implement it because I think it would have been interesting from a multiple imputation perspective, but unfortunately our data was not conducive to that.

Lastly, we move on to the causal analysis part. While propensity score analysis is a popular tool to implement Rubin's causal model, it is not the only one. Other methods like instrumental variables also exist and are used in practice. There are many critiques of the propensity score and the causal method in general. The assumption of ignorability is central to the theory, but this assumption is theoretically untestable. As well, the choice of pretreatment covariates entered into the propensity score model, and the model itself were just chosen for convenience and balancing. While they were both convenient and induced balance, there are certainly other predictors and models that could have achieved the same. This dilemma is a known drawback of propensity score methods. This analysis chose to use propensity score weighting, and while this is theoretically sound, many of the users of propensity scores still prefer matching, because of its historical popularity. And lastly, we can never draw causal inference from a statistical procedure, but we are able to emulate an RCT, so that causal inferences are more justified than in traditional analyses.

An interesting topic for future research would be to use multiple imputation to impute the counterfactual with imputation. After all, the counterfactual framework is actually a missing data problem. It was decided to not use this method here, but using multiple imputation on an already imputed dataset seems like an interesting  topic for further research.
