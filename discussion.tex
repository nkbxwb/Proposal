\chapter{Discussion}
We have discussed a number of tools and methods to analyze survival data with missingness.  There are lots of decisions to be made along the way, and I am in no way advocating that my exact choices are the right ones, I am only claiming that the decisions made were proper for the type of data and questions that we had.
There will certainly be many disagreements about the multiple imputation portion. And since the multiple imputation serves as the root of the analysis, the concerns should be addressed. The first concern comes from people who don’t understand or believe in imputation of missing values. Multiple imputation is a tool to help us find plausible values for missing data. We will make no claim that the imputed values are right, but when used correctly, the results will be unbiased. We aren’t using multiple imputation to create data where there is none, rather we are using it to “fill gaps” in places that we already do have data. We actually need to impute in certain cases if we want to get valid results, as analysis without imputation will lead to severely biased results. For example, if teenage males who are obese don’t want to self-report their weight, then classic available case analysis will yield biased results because we have knowingly left out part of the population who are systematically different \cite{VanBuuren2012}. We need to impute to make sure we have included all of the information and not to bias our estimate.

The next and more substantial critique will come from statisticians who may not believe that the distribution that the imputations is being drawn from is valid. Multiple imputation is inherently a parametric procedure. No matter what method we use to impute, we have to make a parametric assumption, be it the joint model for JM or the full conditionals for FCS.  For our case, using the normal model is certainly wrong because we have so many categorical and strictly positive variavles (which is proven to be suboptimal in \cite{Kropko2014}), so we are left only with using FCS. And FCS alone has weak theoretical justification. But as we have discussed before, many studies have shown that FCS is robust to non-compatibility. As well, there was no formal model validation (such as cross validation), only ad hoc checks. In the literature there is hardly any mention of validation, because if we were to cross validate, we would be drawing from different models, and comparison between the folds would be like comparing apples to oranges. We already have missing data, there is no reason to destabilize it to try to compare it, as the standard methods seem to work fine \cite{VanBuuren2012}.

An interesting extension to this project would be to use a non parametric approach to multiple imputation, such as the one suggested by Long et all in \cite{Long2012}. But at the time of publication, there is not much literature or software on this subject, so I felt that it was not appropriate to use its results.  

To summarize about multiple imputation, I would say that it is a necessary evil. In the process of using multiple imputation, we lose predictive power, and are forced to use a distribution that may not fit the data to a t. But we need to use imputation techniques if we want to make any sense of our data. The advice I would give to those who are hesitant to use multiple imputation would be to not have missing data, but this is a task that is easier said than done. Multiple imputation is becoming the standard for missing data techniques, especially in the medical field. There are lots of pros to it, but there are certainly some conns. Much research has already gone in to it, but much more needs to be done. It is my hope that this thesis has shown a powerful example of why multiple imputation should be used.

Next we can critique the survival section. We decided to use standard Kaplan-Meier and Cox analyses because they are very standard in practice, and answer the questions well. However, some lesser known methods could have been used. A popular theoretical model is called the accelerated failure time model, which describes how covariates affect the hazard, assuming that it acts in a multiplicative fashion. This is useful for clinicians, but not really good for patients, because the conclusions drawn from it are “drug x will make you live 50% longer than y”, and this type of wording conveys more optimism about the model than there really is.  We had some survival data in the dataset that would be appropriate for the competing risks setting, but this was not of primary concert to the clinicians. In future research, analyzing this data would be of interest since it poses many interesting problems.
\begin{comment}
Next, since there is no well established method to validate the model, we had to be creative and define our own methods to check them. The methods are reasonable and both the stacked and individual methods are very similar. More research should be done though to verify if this will always be the case, specifically in the presence of pathological data. 
\end{comment}

There are two concepts  in survival analysis that I find interesting, but our data did not allow for it. The first is variable selection. The clinicians knew what they wanted to test, so this was not needed, but variable selection in the context of MI is an interesting question, and van Buuren covers it in his book \cite{VanBuuren2012}. This would be very useful if our dataset had covariates that we were unsure of their predictive power or wanted to examine. 
Another interesting addition would be using multistate data. In this setting, subjects can transfer from one group to another, ie have cancer, get in to remission, and then relapse.  We model the states as a stochastic process. This would be really interesting, and I would have liked to implement it because I think it would have been interesting from a multiple imputation perspective, but unfortunately our data was not conducive to that.
!!!Work on this a lot more!!! Lastly, we move on to the causal analysis part. While there are many other binary classifiers that could be used to make propensity scores, we chose to use logistic regression. This choice was based solely on tradition and ease of understanding from non statisticians. As well, there has been some new research recently saying that propensity score matching is not as powerful as it was once thought  (? King?). Lastly, our choice of how to combine propensity scores was solely based off of the Mitra paper, and no more studies have been done to show that this is in fact the optimal way to do it.
