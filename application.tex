\chapter{Application}
\section{Data Explanation}
\label{sec:data}
\begin{comment}
We have now laid down the theory of what we want to do describing all of the choices we may need to make, so now we will put it in action. The data that we will apply it to is a dataset from M.D Anderson Cancer Center. The data is a collection of about 1500 patients at MD Anderson who have breast cancer that has metastasized to the brain, about 100 clinically relevant covariates, along with their survival status and time.
\end{comment}
Unfortunately, as of October 16th, we are still waiting for final approval from the PI of the project and the IRB. I want to get the proposal out to y’all though. So, what I will do in this section is describe what I will do, and once I get approval, I will fill in the tables and plots. In the meantime, I will put in <whatever> where something that uses the data should be. The IRB protocol is RCR03-0931, but neither Dr. Hess nor myself are on the protocol, because it is out of date. We are in the process of getting it updated.

Now that we have the theory in place, we can apply it to some real data. The dataset that I chose to analyze is a dataset from MD Anderson Cancer Center, with permission from Dr. Bugano, Dr. Ibrahim, Dr. Hess and <whoever else needs to be thanked>. This dataset has historical records of 1514 MD Anderson patients who have had breast cancer that has metastasized to the brain. There are lots of covariates recorded (about 90, with some missingness), a few different treatments, as well as survival endpoints (which are all observed). This data is exemplary for this task because it is large, survival amenable, has missingness and is a prime candidate for imputation, and has treatment variables that are not given in an RCT. <plot and table  of missingness> 

Our first step is to clearly define what we would like to find. There are many interesting questions we could ask and answer from this data because of the amount of data available, but the question I will focus on here is the effect on survival and treatment of two HER2 (breast cancer grown protein) therapeutic drugs-Lapatinib and Trastuzumab (or do I want to do capecitabine/other/none). It isn’t vital to understand what these drugs do, but the interested reader may want to look at  appendix \ref{app:apdxb} for a very basic overview of cancer and the methods of how these drugs work. For a much more detailed analysis and other clinically relevant questions, see !!Hess, Bugano, Berliner!! This is the project that this research was forked off of, although it will probably not be published by the time this thesis is. 
\section{Imputation}
%start here
We first need to impute the missing data. This is a challenging task, because of the attention and care that needs to be given to imputing about 90 covariates with missing data. But we need these covariates to be imputed properly because; they have the potential to be useful as predictors for other covariates, they might be something we are actually analyzing (now or later), we have spent the money to collect the data, and it strengthens the MAR assumption. As well, it is my opinion (and probably a consensus among applied statisticians) that is better to have too many covariates than not enough. After all, we can always use variable selection if we have too much data. 

Our data is quite high dimensional, and there are a many binary variables, thus JM imputation seems inappropriate. Instead, FCS models seem better suited. We will be using the R package mice \cite{VanBuuren2011} because it is easy to use yet powerful. 

The model is set up by hand, following the advice from \cite{VanBuuren2011}. It took a about three weeks to set up and check. This was because the number of covariates was huge, and checking the imputations after a change was time consuming. It would not take this long for a smaller dataset. Creating valid imputations is a skill that lies somewhere between an art and a scientist, so it takes the theory to know what to do, and trial and error to see if you’ve done it correctly.

The first task we need to do is to assess the missing data mechanism. As we have discussed before, there is no test to determine what the mechanism is. It is very unlikely that the data is MCAR (which we typically associate with random/accidental deletion), so it is between MAR and MNAR. We have so many different covariates, and it could reasonably be assumed that the missing data we have could be explained by the type of disease, its stage, the subject’s age, their standardized assessment, and their survival time, among other things. So it would be reasonable to assume that the missing data mechanism is MAR, and thus imputation can be confidently used

For each covariate with missingness, we need to decide the form of the imputation model that  will be used for imputation, and what predictors will be used in it. I decided to be very forgiving, and use nearly every reasonable predictor for each missing covariate. I did this to bolster the MAR claim, and avoid variable selection. Van Buuren proposes measures called influx and outflux to determine how worthy each covariate will be as a predictor \cite{VanBuuren2012}. I used all predictors except those that had very poor influx and outflux. 

Once the choices had been made, I needed to run the imputations by trial and error. This took a considerable amount of time, because after every change made, I had to rerun the imputation and reassess convergence and validity of the imputation.

For the final imputation, I decided to impute $m=50$ datasets and 40 iterations for each. Mice generally converges quickly (within 5 or 10 iterations), but by setting the number of iterations so high, it is as if we are setting a burn in period, and then taking our sample. The number of datasets was selected to be 50 because in general, according to Reiter the number of datasets should be more than the imputations \cite{Reiter2008}, and while there is no consensus about how many imputations to do, the modern research argues that the more the better. Research by White et. al  says that you should choose $m$ to be about $100*$ the amount of incomplete cases (for the analysis at hand) \cite{White2011a}. We have about 30\% missing data, but coupled with Reiter’s advice, I chose 50.

After the final model for each covariate with missingness has been set up, we need to run it and save the results. For 50 datasets, 40 iterations, the algorithm runs in about X hours, and for 50 datasets with 100 iterations, it took Y hours on a computer with Z ram and Q processors <Get this info from MDACC comps>. While this seems like a long time, this process only needs to be done once and requires no human interaction, so it can be run overnight and then never need to be touched again. I ran it for 100 iterations to see how the run time scaled, as well as to check how the chains behaved and to see how the analyses differed. The results between 50 and 100 imputations were very similar. As well, there is hardly any confidence gained going from 50 to 100, and having such large objects in memory can be harder to work with.

We need to check our final imputations for convergence and reliability. Convergence is assessed by looking at <plots of covariate mean and sd by iteration>. According to van Buuren “the different streams should be freely intermingled with each other, without showing any definite trends. Convergence is diagnosed when the variance between different sequences is no larger than the variance with each individual sequence” \cite{VanBuuren2011}. Looking at these plots, this certainly seems to be the case. Other authors suggest using a more formal statistical tests such as< rhat> to assess convergence, so I also display that (values near 1.00 mean ok and values greater than 1.1 indicate we should run longer). Diagnostic plots are viewed to ensure that the imputed data is similar enough to the real data.< A few of the plots have been replicated here>. To see all of the plots, go to the shiny app/ R package (do this if enough time, also see about security. (Might just need to make it be “available upon request”). As we can see, not all of the imputed data follows the distribution of the observed data exactly, but for the majority of the plots, the data look like they could have been real data.
\section{Survival Analysis}
%start here
Now that the datasets are imputed, we are ready to run our models on them.  As a sanity check, we may compare the fitted models to the available case analysis. Since the imputed values we generate ought to be quite similar to what data we have (unless there is reason to believe that the missing data is significantly different than the observed data, which I don’t believe is the case here), we should expect our estimates to be similar.

It should be noted that in all of our survival analyses, we will be doing a landmark analysis. Landmark analysis means that we don’t start the analysis at time 0, rather, we start it at a different time later than 0. In Dr. Hess’s words, “Since the brain metastases treatment data was necessarily determined after the diagnosis of the brain met, it is not appropriate to use this data as baseline covariates in the analyses. Only covariates known at the time of diagnosis can be used in this fashion… we can do a landmark analysis by estimating when the vast majority of patients would had their brain met treatment choices started and starting our analyses at this point”. After speaking with subject matter experts (Dr. Bugano and Dr. Ibrahim), this landmark time was determined to be 2 months. 

The first result that we will check is the Kaplan-Meier curves for the imputed data. The available case analysis shows that lapatinib and trastuzumab are quite close to each other, while having no HER2  directed treatment being much lower. The log rank test statistic is X, with pvalue Y, indicating Z. The pooled KM estimate was found using Rubin’s rules, but under a complimentary log-log transform as suggested by \cite{Marshall2009} to get the survival curves towards normality. The results from MI look quite similar <AC analysis and MI analysis>. We can also get an approximation for the log-rank test on the MI data via the Wald test on the pooled Cox model <with results in table here>. Recall that we are not able to get the exact log-rank test because in doing so, we would need to compute either the likelihood ratio test or score test, both of which would include calculating the risk set, which is not possible in the MI case. Another suggestion is to pool the chi square statistics via methods presented in Marshall et al 2009, but even they say that this method is poor \cite{Marshall2009}. So, our only real option is to use the Wald test (which is very easy to compute), and use that value as a proxy for the log rank test (they are asymptotically equivalent).

Now that we have estimate of the survival curve, we may set up a model to observe how changes in some baseline covariates change the hazard. We will do this with the Cox Proportional Hazards model. Once we have a baseline model fit and the assumptions met, we can add our treatment variable to see how this affects the hazard.  We first need to fit a reasonable model on the available cases .The available case model  <will be seen in the table below>.  We need to make sure that the proportional hazards assumption is met in the available case model so we can apply it to the MI data. To check, we visually inspect the Schoenfeld residuals over time, and check the test of correlation between the residuals and time. Overall, the assumption of proportional hazards over time seems reasonable, and the test statistic affirms this <AC cox.zph plots>. Although the splines fit the points may not look straight, it certainly seems reasonable that a straight line could be fit between the 95% confidence bands.

We have verified that the available case analysis seems reasonable, now we can work with the MI data. We fit that same Cox model on all of our imputed data sets, and pool our results via Rubin’s rules. We need to verify that we still have proportional hazards though. This is not a straight forward task anymore, since we don’t actually have a model, rather, we have the average of multiple models. We are no longer estimating the parameters by maximizing the partial likelihood; rather we are estimating them based on the average of the coefficients from the MI datasets. There are two ways we can go about verifying the proportional hazards assumption. 

The first is to check the proportional hazards assumptions on the stacked dataset.  To do so, we plot the Schoenfeld residuals over time, and observe the spline fit to it (which should be independent of time). This is a good visual tool, but when running the chi square test to check for the correlation between the coefficient and time, the sample will be artificially too big, and thus we cannot trust the results. 

The correct way to do this is to observe each plot and statistic generated from the 50 datasets to see if the assumptions hold. This may seem like an arduous task when the number of imputed datasets is large, but we can circumvent it by writing <a shiny app to view them>, or <plot all of the splines on one plot>. We can also look at all of the chi square tests for the 50 datasets and 13 variables for each, although there is bound to be some overlap between significance and non-significance due to the multiple testing problem. Overall, our imputed plots are very similar to the plots produced by complete case analysis, to which we have deemed to be acceptable for the proportional hazards assumption. We may now look at the Cox regression coefficients and exponentiate them in order to obtain the hazard ratios, and obtain corresponding 95\% confidence intervals. Looking at <!! table whatever!!> , we can see that some factors  (such as X, Y, Z) force a larger hazard ratio than others. 

We may then add in our treatment variable to see how it affects the hazard, and see how it changes other factors. After doing so, we can see that X does Y. The results can be seen here in a table <of AC vs MI estimates>.

\section{Causal analysis}

Lastly, we will want to draw causal inference, and see what the average treatment effect of each drug is. This is necessary because the data was collected from a database, and we did not have a completely randomized experiment.  As well, this piece of information is what clinicians and laypeople really want—it answers the question of which drug is better. There are many interesting questions that we may ask with this dataset, but here we will only focus on lapatinib vs trastuzumab vs no treatment. The interested reader may read !!my paper!! Upon its publication. The idea for this part of the analysis is to use propensity scores to match subjects and then compare them. As we saw earlier the best way to match is using the X method. There are several R packages to do propensity score matching in R, including X Y Z . I chose to use the X package because of its ease of use. Do a lot more work on this part!!!!!
