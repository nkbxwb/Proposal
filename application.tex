\chapter{Application}
\section{Data Explanation}
\label{sec:data}
We have now laid down the theory of what we want to do, so now we will put it in action. The data that we will apply it to is a dataset from M.D Anderson. The data is a collection of about 1500 patients at MD Anderson who have breast cancer that has metastasized to the brain, about 100 clinically relevant covariates, along with their survival status and time.

Unfortunately, as of October 5th, we are still waiting for IRB approval to use the data . I want to get the proposal out to y’all though. So, what I will do in this section is describe what I will do, and once I get approval, I will fill in the tables and plots. In the meantime, I will put in <whatever> where something that uses the data should be.

Now that we have the theory in place, we can apply it to some real data. The dataset that I chose to analyze is a dataset from MD Anderson Cancer Center, with permission from Dr. Bugano, Dr. Ibrahim, Dr. Hess and <whoever else needs to be thanked>. This dataset has historical records of X MD Anderson patients who have had breast cancer that has metastasized to the brain. There are lots of covariates recorded (about 90), a few different treatments, as well as survival endpoints. This data is exemplary for this task because it is large, survival amenable, has missingness and is a prime candidate for imputation, and has treatment variables that are not given in an RCT.

Our first step is to define what we would like to find. There are many interesting questions we could ask and answer from this data because of the amount of data available, but the question I will focus on here is the effect on survival and treatment of two HER2 (breast cancer grown protein) therapeutic drugs-Lapatinib and Trastuzumab. It isn’t vital to understand what these drugs do, but the interested reader may want to look at appendix !! b?!! for a more detailed  look at these. For a much more detailed analysis and other clinically relevant questions, see !!Hess, Bugano, Berliner!! This is the project that this research was forked off of, although it will probably not be published by the time this thesis is. 

We first need to impute the missing data. This is a little challenging just because of the sheer number of covariates that we have (about 90? with missingness). But we need these covariates, because they have the potential to be useful as predictors for other covariates, they might be something we are actually analyzing, we have spent the money to collect the data, and it strengthens the MAR assumption. As well, it is my opinion (and probably a consensus among applied statisticians) that is better to have too many covariates than not enough. After all, we can always use variable selection if we have too much data. 
%start here
The model is set up, and the appropriate methods are selected for each datatype. The mice algorithm from the R package mice is run. For 50 datasets, 40 iterations, the algorithm runs in about X hours. While this seems like a long time, this only needs to be done once.
Convergence is assessed, and diagnostic plots are viewed to ensure that the imputed data is similar enough to the real data. A few of the plots have been replicated here. To see all of the plots, go to the shiny app (do this if enough time).  Not all of the imputed data follows the distribution of the observed data exactly, but we obviously don’t expect this to happen.
Now that the datasets are imputed, we are ready to run our models on them.  As a sanity check, we may compare them to available case analysis. Since the imputed values we generate ought to be quite similar to what data we have, we should expect our estimates to be similar.
The first result that we will check is the Kaplan Meier curves for the imputed data. The available case analysis seems to show that lapatinib and trastuzumab are quite close to each other, with no treatment being much lower. The results from MI look quite similar. [put the stuff in]. The pooled KM estimate was found using Rubin’s rules, but under a cloglog transform as suggested by \cite{Marshall2009} to get towards normality.  We can also run a log-rank test on the MI data. This was implemented by \cite{Zhao2014} using another form of imputation called kmmi, but it has not ever been used on regular MI (kmmi works on missing censoring times). Log rank test is a normally distributed quantity asymptotically, so we can just pool it as normal and use the degrees of freedom from Rubin and Barnard to get our inference.  !! put the analysis here!!
!!!Do I want to do competing risks analysis?!!!
Now that we have estimate of survival, we may set up a model to observe how changes in some baseline covariates change the hazard. To do this, we need to run a Cox proportional hazards model. The original available case model is as follows.  We need to make sure that the proportional hazards assumption is met, so we may check the cox zph command to look at the schoenfeld residuals over time, and check the test stat. Overall, it looks to be proportional hazards over time, and the test statistic affirms this.  Then, we fit that same cox model on all of our imputed data sets, and pool our results via Rubin’s rules (no transformation needs to be done since the cox model coefficients assume asymptotical normality). We need to verify that we still have proportional hazards though. This is not an easy task, since we don’t actually have a model, rather, we have the average of multiple models. We are no longer estimating the parameters by maximizing the partial likelihood, rather we are estimating them based on the average of the coefficients from the MI datasets. There are two ways we can go about this. The first is to check the proportional hazards assumptions on the stacked dataset.  This will give us a good visualization about the shape of the proportional hazards over time, but when running the chi square test to check for the correlation between the coefficient and time, the sample will be artificially too big, and thus we cannot trust the results. The correct way to do this is to observe each plot and statistic generated from the m datasets to see if the assumptions hold. This may seem like an arduous task when the number of imputed datasets is large, but we can circumvent it by writing a shiny app to view them, or plot all of the loess curves on one plot. We can also get the average of the chi square test results if we need a little more information than looking at the plots. Overall though, our imputed plots are very similar to the plots produced by complete case analysis, to which we have deemed to be acceptable for the proportional hazards assumption. We may now look at the cox regression coefficients and exponentiate them in order to obtain the hazard ratios. Looking at !! table whatever!! , we can see that some factors force a larger hazard ratio than others. We can take the reciprocal of it to look at the protective effects of each covariate.
Lastly, we will want to draw causal inference, and see what the average treatment effect of each drug is. This is necessary because the data was collected from a database, and we did not have a completely randomized experiment.  As well, this piece of information is what clinicians and laypeople really want—it answers the question of which drug is better. There are many interesting questions that we may ask with this dataset, but here we will only focus on lapatinib vs trastuzumab vs no treatment. The interested reader may read !!my paper!! Upon its publication. The idea for this part of the analysis is to use propensity scores to match subjects and then compare them. As we saw earlier the best way to match is using the X method. There are several R packages to do propensity score matching in R, including X Y Z . I chose to use the X package because of its ease of use. Do a lot more work on this part!!!!!
