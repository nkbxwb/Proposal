\chapter{Application}
\section{Data Explanation}
\label{sec:data}
We have now laid down the theory of what we want to do, so now we will put it in action. The data that we will apply it to is a dataset from M.D Anderson. The data is a collection of about 1500 patients at MD Anderson who have breast cancer that has metastasized to the brain, about 100 clinically relevant covariates, along with their survival status and time.

Unfortunately, as of October 5th, we are still waiting for IRB approval to use the data . I want to get the proposal out to y’all though. So, what I will do in this section is describe what I will do, and once I get approval, I will fill in the tables and plots. In the meantime, I will put in <whatever> where something that uses the data should be.

Now that we have the theory in place, we can apply it to some real data. The dataset that I chose to analyze is a dataset from MD Anderson Cancer Center, with permission from Dr. Bugano, Dr. Ibrahim, Dr. Hess and <whoever else needs to be thanked>. This dataset has historical records of X MD Anderson patients who have had breast cancer that has metastasized to the brain. There are lots of covariates recorded (about 90, with some missingness), a few different treatments, as well as survival endpoints (which are all observed). This data is exemplary for this task because it is large, survival amenable, has missingness and is a prime candidate for imputation, and has treatment variables that are not given in an RCT. <plot or table  of missingness> 

Our first step is to define what we would like to find. There are many interesting questions we could ask and answer from this data because of the amount of data available, but the question I will focus on here is the effect on survival and treatment of two HER2 (breast cancer grown protein) therapeutic drugs-Lapatinib and Trastuzumab. It isn’t vital to understand what these drugs do, but the interested reader may want to look at appendix !! b?!! for a more detailed  look at these. For a much more detailed analysis and other clinically relevant questions, see !!Hess, Bugano, Berliner!! This is the project that this research was forked off of, although it will probably not be published by the time this thesis is. 

We first need to impute the missing data. This is a little challenging just because of the sheer number of covariates that we have (about 90? with missingness). But we need these covariates, because they have the potential to be useful as predictors for other covariates, they might be something we are actually analyzing, we have spent the money to collect the data, and it strengthens the MAR assumption. As well, it is my opinion (and probably a consensus among applied statisticians) that is better to have too many covariates than not enough. After all, we can always use variable selection if we have too much data. 

Our data is quite high dimensional, and there are a many binary variables, thus JM modeling seems inappropriate. Instead, FCS models seem better suited. We will be using the R package mice \cite{VanBuuren2011} because it is easy to use yet powerful. 

The model is set up by hand. For each covariate with missingness, we need to decide what method will be used for imputation, and what predictors will be used in it. I decided to be very forgiving, and use nearly every predictor for each missing covariate. I did this to bolster the MAR claim, and avoid variable selection. As well, the appropriate methods need to be selected for each datatype. The majority of the data is categorical, so decisions need to be made about whether to impute them via predictive mean matching or logistic regression. This decision was made by observing density plots after the algorithm was run to see if the imputations for each kind were valid. As well, derived variables were coded in as passive imputation, so that they would not be imputed, rather they would be computed.

After the model has been set up, we need to run it and save the results. For 50 datasets, 40 iterations, the algorithm runs in about X hours, and for 50 datasets with 100 iterations, it took Y hours on a computer with Z ram and Q processors. While this seems like a long time, this process only needs to be done once and requires no human interaction, so it can be run overnight and then never need to be touched again. For the rest of the analysis, I choose to use the 50 datasets, because there is hardly any confidence going from 50 to 100, and having such large objects in memory can be harder to work with.
Once the mice algorithm has run, we need to check for convergence and reliability. Convergence is assessed by looking at <plots of covariate mean and sd by iteration>. According to van Buuren “the different streams should be freely intermingled with each other, without showing any definite trends. Convergence is diagnosed when the variance between different sequences is no larger than the variance with each individual sequence” \cite{VanBuuren2011}. Looking at these plots, this certainly seems to be the case. Other authors suggest using a more formal statistical tests such as <rhat> to assess convergence, so I also display that (values near 1.00 mean ok and values greater than 1.1 indicate we should run longer). Diagnostic plots are viewed to ensure that the imputed data is similar enough to the real data.< A few of the plots have been replicated here>. To see all of the plots, go to the shiny app/ R package (do this if enough time, also see about security. Might just need to make it be “available upon request”). As we can see, not all of the imputed data follows the distribution of the observed data exactly, but we obviously don’t expect this to happen always. For the majority of the plots though, the data look like they could have been real data.

Now that the datasets are imputed, we are ready to run our models on them.  As a sanity check, we may compare the fitted models to the available case analysis. Since the imputed values we generate ought to be quite similar to what data we have (unless there is reason to believe that the missing data is significantly different than the observed data), we should expect our estimates to be similar.

It should be noted that in all of our survival analyses, we will be doing a landmark analysis. Landmark analysis means that we don’t start the analysis at time 0, rather, we start it at a different time. In Dr. Hess’s words, “Since the brain met treatment data was necessarily determined after the diagnosis of the brain met, it is not appropriate to use this data as baseline covariates in the analyses. Only covariates known at the time of diagnosis can be used in this fashion… we can do a landmark analysis by estimating when the vast majority of patients would had their brain met treatment choices started and starting our analyses at this point”. After speaking with subject matter experts (Dr. Bugano and Dr. Ibrahim), this landmark time was determined to be 2 months. 

The first result that we will check is the Kaplan Meier curves for the imputed data. The available case analysis shows that lapatinib and trastuzumab are quite close to each other, while having no her2 directed treatment being much lower. The logrank test statistic is X. The pooled KM estimate was found using Rubin’s rules, but under a complimentary log-log transform as suggested by \cite{Marshall2009} to get the survival curves towards normality. The results from MI look quite similar <AC analysis and MI analysis>. We can also get an approximation for the log-rank test on the MI data via the Wald test on the pooled Cox model (NOT the Kaplan-Meier model, Kaplan Meier is not normally distributed, and no obvious transformation exists to make it so, so we must use Cox). We are not able to get the exact log-rank test because in doing so, we would need to compute either the likelihood ratio test or score test, both of which would include calculating the risk set, which is not possible in the MI case. Another suggestion is to pool the chi square statistics via methods presented in Marshall et al 2009, but even they say that this method is poor \cite{Marshall2009}. So, our only real option is to use the wald test (which is very easy to compute), and use that value as a proxy for the log rank test (they are asymptotically equivalent).

!!!Do I want to do competing risks analysis? I will if I have time!!!

Now that we have estimate of survival, we may set up a model to observe how changes in some baseline covariates change the hazard. We will do this with the Cox Proportional Hazards model. Once we have a baseline model fit and the assumptions met, we can add our treatment variable to see how this affects the hazard. The original available case model is as follows.  We need to make sure that the proportional hazards assumption is met, so we may check the cox zph command to look at the schoenfeld residuals over time, and check the test stat. Overall, the assumption of proportional hazards over time seems reasonable, and the test statistic affirms this <AC cox.zph plots>. Although the splines fit the points may not look straight, it certainly seems reasonable that a straight line could be fit (denoting a hazard that is constant over time) between the 95% confidence bands (this procedure doesn’t have a name, but it is known colloquially as the straight edge test).  

Now that we have verified that the available case analysis seems reasonable, we can work with the MI data. We fit that same cox model on all of our imputed data sets, and pool our results via Rubin’s rules (no transformation needs to be done since the Cox model coefficients are assumed to be asymptoticaly normal). We need to verify that we still have proportional hazards though. This is not an easy task, since we don’t actually have a model, rather, we have the average of multiple models. We are no longer estimating the parameters by maximizing the partial likelihood; rather we are estimating them based on the average of the coefficients from the MI datasets. There are two ways we can go about this. The first is to check the proportional hazards assumptions on the stacked dataset.  This will give us a good visualization about the shape of the splines fit to the residuals over time, but when running the chi square test to check for the correlation between the coefficient and time, the sample will be artificially too big, and thus we cannot trust the results. The correct way to do this is to observe each plot and statistic generated from the m datasets to see if the assumptions hold. This may seem like an arduous task when the number of imputed datasets is large, but we can circumvent it by writing a shiny app to view them, or <plot all of the splines on one plot>. We can also look at all of the chi square tests for the 50 datasets and 13 variables for each, although there is bound to be some overlap between significance and non significance due to the multiple testing problem. Overall though, our imputed plots are very similar to the plots produced by complete case analysis, to which we have deemed to be acceptable for the proportional hazards assumption. We may now look at the cox regression coefficients and exponentiate them in order to obtain the hazard ratios. Looking at !! table whatever!! , we can see that some factors force a larger hazard ratio than others. We can take the reciprocal of it to look at the protective effects of each covariate. We may then add in our treatment variable to see how it effects the hazard, and see how it changes other factors.
Lastly, we will want to draw causal inference, and see what the average treatment effect of each drug is. This is necessary because the data was collected from a database, and we did not have a completely randomized experiment.  As well, this piece of information is what clinicians and laypeople really want—it answers the question of which drug is better. There are many interesting questions that we may ask with this dataset, but here we will only focus on lapatinib vs trastuzumab vs no treatment. The interested reader may read !!my paper!! Upon its publication. The idea for this part of the analysis is to use propensity scores to match subjects and then compare them. As we saw earlier the best way to match is using the X method. There are several R packages to do propensity score matching in R, including X Y Z . I chose to use the X package because of its ease of use. Do a lot more work on this part!!!!!
