\chapter{Application}
\section{Data Explanation}
\label{sec:data}
Now that we have the theory in place, we can apply it to some real data. The dataset that I chose to analyze is a dataset from MD Anderson cancer center, with permission from Dr. Bugano (get the permission!!). This dataset has historical records of X MD anderson patients who have had breast cancer that has metastized to the brain, and it records many covariate, treatments, as well as survival endpoints. This data is exemplary for this task because it is large, survival amenable, has missingness that is easy to impute on, and has treatment variables.
Our first step is to define what we would like to find. There are many interesting questions we could ask from this data because of the amount of data available, but the question I will focus on here is the effect on survival and treatment of two HER2 theraputic drugs-Lapatinib and Trastuzumab. For a much more detailed analysis and other clinically relevant questions, see !!Hess, Bugano, Berliner!!. So, we will want to check out survival curves and cox regression, as well as analze the treatment effect.
We first need to impute the missing data. This is a little challenging just because of the sheer number of civariates that we have. But we need these covariates. With more covariates, the more sure we can be in the assumption of MAR missingness. As well, it is better to have too many covariates than not enough. The model is set up, and the appropriate methods are selected for each datatype. The mice algorithm from the R package mice is run. For 50 datasets, 40 iterations, the algorithm runs in about X hours. While this seems like a long time, this only needs to be done once.
Convergence is assessed, and diagnostic plots are viewed to ensure that the imputed data is similar enough to the real data. A few of the plots have been replicated here. To see all of the plots, go to the shiny app (do this if enough time).  Not all of the imputed data follows the distribution of the observed data exactly, but we obviously don’t expect this to happen.
Now that the datasets are imputed, we are ready to run our models on them.  As a sanity check, we may compare them to available case analysis. Since the imputed values we generate ought to be quite similar to what data we have, we should expect our estimates to be similar.
The first result that we will check is the Kaplan meier curves for the imputed data. The available case analysis seems to show that lapatinib and trastuzumab are quite close to eachother, with no treatment being much lower. The results from MI look quite similar. [put the stuff in]. The pooled KM estimate was found using rubins rules, but under a cloglog transform as suggested by \cite{Marshall2009} to get towards normality.  We can also run a logrank test on the MI data. This was implemented by \cite{Zhao2014} using another form of imputation called kmmi, but it has not ever been used on regular MI (kmmi works on missing censoring times). Log rank test is a normally distributed quantity asymptotically, so we can just pool it as normal and use the degrees of freedom from Rubin and barnard to get our inference.  !! put the analysis here!!
!!!Do I want to do competing risks analysis?!!!
Now that we have estimate of survival, we may set up a model to observe how changes in some baseline covariates change the hazard. To do this, we need to run a Cox proportional hazards model. The original available case model is as follows.  We need to make sure that the proportional hazards assumption is met, so we may check the cox zph command to look at the schoenfeld residuals over time, and check the test stat. Overall, it looks to be proportional hazards over time, and the test statistic affirms this.  Then, we fit that same cox model on all of our imputed data sets, and pool our results via rubins rules (no transformation needs to be done since the cox model coefficients assume asymptotical normality). We need to verify that we still have proportional hazards though. This is not an easy task, since we don’t actually have a model, rather, we have the average of multiple models. We are no longer estimating the parameters by maximizing the partial likelihood, rather we are estimating them based on the average of the coefficients from the MI datasets. There are two ways we can go about this. The first is to check 
